{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:PythonData]",
      "language": "python",
      "name": "conda-env-PythonData-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/So-AI-love/ReinformentLearning-QuantumWellStatePreparation/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I1I-_HU-kCn"
      },
      "source": [
        "!pip install qutip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aor5kb7q-e7P",
        "outputId": "0b6bda2e-d561-4c4e-b7e9-dada581a4a7d"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import pi\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "from qutip import *\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import interp1d"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/jyj/anaconda3/envs/PythonData/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOSIJIt2-e7b"
      },
      "source": [
        "#Solve the eigen-states and overlappings(see fig3 and fig4 in arXive:1812.04381) \n",
        "import numpy as np\n",
        "from math import sin,pi \n",
        "import math\n",
        "import scipy.optimize\n",
        "from scipy.optimize import fsolve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "N_Keep=6 #the number of energy levels we keep. Higher levels are neglected.\n",
        "E0=pi**2/2 #gound state energy in natural unit system, used to rescale energy and alpha\n",
        "\n",
        "def generate_bands_and_overlap(epsilon=0.01):\n",
        "        a=1/2+epsilon\n",
        "        b=1/2-epsilon\n",
        "        \n",
        "        def solve(alpha,solution):\n",
        "            def F(k):\n",
        "                c=2*alpha\n",
        "                return k*np.sin(k*(a+b))+c*np.sin(k*a)*np.sin(k*b)\n",
        "            solution=fsolve(F,solution) \n",
        "            return solution\n",
        "\n",
        "       \n",
        "        solution=np.array([s*pi for s in range(1,N_Keep+1)]) #without Dirac function\n",
        "        Energies=[]\n",
        "        kn=[]\n",
        "        AS=[]\n",
        "       \n",
        "        alpha_list=np.array(range(10000))*0.2*E0 ##choose alpha list\n",
        "        for alpha in alpha_list:\n",
        "            solution=solve(alpha,solution)\n",
        "            energy=[s**2/2/E0   for s in solution]  #wave vector to energy\n",
        "            As=[(a/2-np.sin(2*k*a)/(4*k)+np.sin(k*a)**2/np.sin(k*b)**2*(b/2-np.sin(2*k*b)/(4*k)))**-1 for k in solution]\n",
        "            Energies.append(energy)\n",
        "            AS.append([np.sqrt(abs(s)) for s in As])\n",
        "            kn.append(solution)\n",
        "     \n",
        "        Energies=np.matrix(Energies)\n",
        "        kn=np.matrix(kn)\n",
        "        AS=np.matrix(AS)\n",
        "        alpha_list/=E0  \n",
        "        #on the above, we rescale energy and alpha_list for plotting purpose, in the following, \n",
        "        #let's undo the rescale\n",
        "\n",
        "        ###store the eig data####:\n",
        "        Energies=Energies*E0\n",
        "        alpha_list=alpha_list*E0 \n",
        "        overlap=np.zeros((len(kn),N_Keep,N_Keep))\n",
        "        for k in range(len(kn)):\n",
        "            am=np.matrix(np.zeros((N_Keep,N_Keep)))\n",
        "            for s1 in range(N_Keep):\n",
        "                for s2 in range(N_Keep):\n",
        "                    if s1!=s2:\n",
        "                        dn=(Energies[k,s2]-Energies[k,s1])\n",
        "                        up=AS[k,s2]*np.sin(kn[k,s2]*a) * AS[k,s1]*np.sin(kn[k,s1]*a)\n",
        "                        ratio=up/dn\n",
        "                        am[s1,s2]=ratio\n",
        "            overlap[k,:,:]=am\n",
        "      \n",
        "        np.savetxt('./data/alpha_list_e_'+'{:.3f}'.format(epsilon)+'.txt', alpha_list, delimiter=',')\n",
        "        np.savetxt('./data/Energies_e_'+'{:.3f}'.format(epsilon)+'.txt', Energies, delimiter=' ')\n",
        "        np.savetxt('./data/overlap_e_'+'{:.3f}'.format(epsilon)+'.txt', overlap.flatten(), delimiter=' ')\n",
        "        return alpha_list,Energies, overlap\n",
        "\n",
        "N_asymmetries=6\n",
        "for n in range (N_asymmetries):\n",
        "    #     asymmetry = round(0.53 + 0.002*n,3)\n",
        "    asymmetry=round(0.03+0.002*n,3)\n",
        "    alpha_list,Energies, overlap=generate_bands_and_overlap(epsilon=asymmetry)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNrEgOTi-e7c"
      },
      "source": [
        "# AS0,kn0,alpha_list0,Energies0, overlap0=generate_bands_and_overlap(epsilon=0.01), the fig3 and fig4 in arXive:1812.04381\n",
        "# were reproduced exactly!  Helpful codes in SolveEig.ipynb, or, use the following lines for Fig3:\n",
        "#alpha_list=alpha_list0/E0;alpha_list=alpha_list[:100]\n",
        "# Energies=Energies0/E0;Energies=Energies[:100]\n",
        "# kn=kn0[:100]\n",
        "# AS=AS0[:100]\n",
        "# %matplotlib inline\n",
        "# _=plt.plot(alpha_list,  Energies[:,0], 'r--', alpha_list,  Energies[:,1], 'b-', \n",
        "#            alpha_list, Energies[:,2], 'g-',alpha_list,  Energies[:,3], 'y-')\n",
        "# _=plt.ylabel('F function')\n",
        "# plt.show()\n",
        "# epsilon=0.01\n",
        "# a=1/2+epsilon\n",
        "# amp=[]\n",
        "# for k in range(len(kn)):\n",
        "#     am=[]\n",
        "#     for s in range(1,6):\n",
        "#         dn=(Energies[k,s]-Energies[k,0])*E0\n",
        "#         up=AS[k,s]*np.sin(kn[k,s]*a) * AS[k,0]*np.sin(kn[k,0]*a)\n",
        "#         ratio=up/dn\n",
        "#         am.append(ratio)\n",
        "#     amp.append(am)\n",
        "# amp=np.matrix(amp)\n",
        "# amp=abs(amp)\n",
        "# _=plt.plot(alpha_list, amp[:,0],'b*-',alpha_list, amp[:,1], 'r--', alpha_list, amp[:,2], 'b-', \n",
        "#            alpha_list,amp[:,3], 'g*-',alpha_list, amp[:,4], 'y-')\n",
        "# _=plt.ylabel('F function')\n",
        "# plt.ylim(0, 0.01)\n",
        "# plt.xlim(0, 20)\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRx4NuT8-e7c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSZDKv6O-e7d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cspbiBjm-e7d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmHnMV3R-e7d"
      },
      "source": [
        "class Szilard_env:\n",
        "    \n",
        "    N_keep=6\n",
        "    def __init__(self):\n",
        "\n",
        "        self.max_lvl = self.N_keep\n",
        "        \n",
        "        self.viewer = None\n",
        "        self.action_space = np.array([-1024,-512,-256,-128,-64,-32,-16,-8,-4,-2,0\n",
        "                                      ,2,4,8,16,32,64,128,256,512,1024])*10\n",
        "        self.action_space = np.reshape(self.action_space, (-1,1))\n",
        "        \n",
        "        self.state_space = np.array([0,0])\n",
        "        self.state_space = np.reshape(self.state_space, (-1,1))\n",
        "\n",
        "        self.max_alfa = 2000.0*E0       #set the maximun alfa \n",
        "        self.t_max = 5                 #time duration:[0,T], T=t_max\n",
        "        self.N_alfa = 50               #set #alpha = #state= #time moments= #actions\n",
        "        self.alfa_dt = self.t_max/self.N_alfa    #time interval\n",
        "        \n",
        "        self.sigma1 = 0.05    # punishment for finite probs of excited states with n>=3 \n",
        "        self.sigma2 = 0.05    # punishment for different probs of the lowest two states\n",
        "        \n",
        "        self.best_protocol = [None]*self.N_alfa        \n",
        "        self.protocol = [None]*self.N_alfa             \n",
        "        self.best_reward = 0                        #best_reward和score的关系是啥 \n",
        "        self.scores = []   #memorize reward for each episode when it is ended.\n",
        "        \n",
        "        self.interp_energies = []      \n",
        "        self.interp_overlaps = []\n",
        "        k = 0\n",
        "        \n",
        "        self.N_asymmetries = 6\n",
        "        self.asymmetries = []\n",
        "        \n",
        "        self.N_t=1000; #for each episode, N_t is # of discret time we use in wave function evolution\n",
        "        \n",
        "        for n in range (self.N_asymmetries):\n",
        "            asymmetry = round(0.53 + 0.002*n,3)\n",
        "            self.asymmetries.append(asymmetry)                     #考虑哪些个不对称性的case\n",
        "            \n",
        "            epsilon=asymmetry-0.5\n",
        "            energy, overlap = self.interp_generator(epsilon)     #d对每个不对称case, 生成能级和交叠作为时间的差值函数（generator形式）\n",
        "            self.interp_energies.append(energy)      \n",
        "            self.interp_overlaps.append(overlap)\n",
        "        \n",
        "        ## Here we can change the reward distribution to favour certain asymmetries\n",
        "        \n",
        "        #r_min  = 0.50\n",
        "        #N_0 = -self.N_asymmetries/np.log(r_min)\n",
        "        #x = np.array([n for n in range (self.N_asymmetries)])\n",
        "        #self.reward_distribution = np.exp(-x/N_0)\n",
        "        self.reward_distribution = np.ones(self.N_asymmetries)\n",
        "            \n",
        "    ## Interpolate data files to calculate eigenstates of the single-particle-box.  \n",
        "    \n",
        "    def interp_generator(self,epsilon):\n",
        "        N_keep=self.N_keep\n",
        "        #给定不对称位置，N_keep个能量和波函数叠加作为时间函数的数据，再生成差值函数\n",
        "        \n",
        "        tab_alfa = np.loadtxt('./data/alpha_list_e_'+'{:.3f}'.format(epsilon)+'.txt', delimiter = ',')        \n",
        "        energies = np.loadtxt('./data/Energies_e_'+'{:.3f}'.format(epsilon)+'.txt', delimiter = ' ')\n",
        "        overlap = np.loadtxt('./data/overlap_e_'+'{:.3f}'.format(epsilon)+'.txt', delimiter = ' ')\n",
        "        overlap = np.reshape(overlap, [max_lvl,max_lvl,-1], order = 'F')\n",
        "        overlap=np.transpose(overlap,(1,0,2))\n",
        "\n",
        "        interp_energy = [None]*N_keep\n",
        "\n",
        "        for n in range(N_keep):\n",
        "            f = interp1d(tab_alfa,energies[:,n], kind = 'cubic')   #给定n‘th level,有一系列的能量对应一系列的alpha值（不同时刻）\n",
        "            interp_energy[n] = f  #这里的f是一个函数“returns a function whose call method uses interpolation to find the value of new points.”\n",
        "\n",
        "        interp_overlap = [[0] *N_keep for i in range(N_keep)] #给定n'th level,它和另外的每一个level之间都有overlap\n",
        "\n",
        "        for n in range(N_keep):\n",
        "            for m in range(N_keep):\n",
        "                interp_overlap[n][m] = interp1d(tab_alfa,overlap[n,m,:], kind = 'cubic') #对应每两个level,对每一个时刻（或alpha的值）都有overlap的值\n",
        "\n",
        "        return interp_energy, interp_overlap\n",
        "    \n",
        "    ## Interpolate the discrete protocol obtained from the agent at the end of each episode, and calculate the reward.\n",
        "    def interp_reward(self, N_t):\n",
        "\n",
        "        time_interp = np.array([ n*self.alfa_dt for n in range (self.N_alfa+1) ]) #线性时间\n",
        "        \n",
        "        alfa_interp = interp1d( time_interp,self.protocol, kind = 'cubic')  #protocol作为时间的函数进行差值\n",
        "      \n",
        "        time = np.linspace(0,self.t_max,N_t)       #这里N_t可以大大于N_alfa吧   \n",
        "        alfa = alfa_interp(time)                     #\n",
        "        alfa = np.clip(alfa, 0, self.max_alfa)   #限制alpha在[0,5000]之间, 对应函数interaction_int里面的alfa_in\n",
        "    \n",
        "        dt = time[1]-time[0]  \n",
        "        max_lvl = self.max_lvl\n",
        " \n",
        "        E = []\n",
        "        for t in range(N_t):\n",
        "            E.append([self.interp_energy[n](alfa[t]) for n in range(max_lvl)]) #self.interp_energy 是给定asymmetry的差值函数\n",
        "        E = np.array(E)\n",
        "        \n",
        "        phi = []\n",
        "        for t in range(N_t):\n",
        "                integral = [np.trapz(x=time[0:t+1], y= E[0:t+1,n]) for n in range(max_lvl)]\n",
        "                phi.append(integral)  #对应每个时刻，都可以求出相位表达式，即文2的appendix: A(4)\n",
        "        phi = np.array(phi)   \n",
        "\n",
        "        def interaction_int(n, alfa_in, f = []):\n",
        "            test = sum([self.interp_overlap[n][m](abs(alfa_in))*np.exp(1j*(phi[t,n] - phi[t,m]) )*f[m] for m in range(max_lvl) if m != n ])*dalfa\n",
        "            return test  #根据我的推导，这里的interp_overlap[n][m](abs(alfa_in))=<psi(n)|dH/dt|psi(m)>/(E_n-E_m)\n",
        "        ##问题是，作为函数变量的alfa_in怎么能提供dH/dt里面的alpha对时间求导项？最后乘的dalfa？  这里需要乘一个\n",
        "\n",
        "        C = np.array([ 0 for n in range(max_lvl) ])\n",
        "        C[0] = 1\n",
        "        C_new = np.array( [ 0 for n in range(max_lvl) ])\n",
        "        C_t = []\n",
        "        C_t.append(C)\n",
        "\n",
        "        for t in range(N_t -1):\n",
        "            \n",
        "            dalfa = (alfa[t+1] - alfa[t])/dt   #alpha的导数\n",
        "\n",
        "            k1 = np.array([dt*interaction_int(n, alfa[t], C ) for n in range(max_lvl)])\n",
        "            k2 = np.array([dt*interaction_int(n, alfa[t]+ dalfa*dt/2, C + k1/2 ) for n in range(max_lvl)])\n",
        "            k3 = np.array([dt*interaction_int(n, alfa[t]+ dalfa*dt/2, C + k2/2 ) for n in range(max_lvl)])\n",
        "            k4 = np.array([dt*interaction_int(n, alfa[t]+ dalfa*dt, C + k3) for n in range(max_lvl)])\n",
        "\n",
        "            C_new = C + (k1 + 2*k2 + 2*k3 + k4)/6   #这是某种迭代方法\n",
        "\n",
        "            C = C_new\n",
        "\n",
        "            C_t.append(C)#有必要把每个时刻的波函数都保存下来吗？\n",
        "\n",
        "        c1 = abs(C[0])**2\n",
        "        c2 = abs(C[1])**2\n",
        "        \n",
        "        reward = np.exp( -((c1 + c2 - 1))**2/self.sigma1 - ((c1 - c2))**2/self.sigma2 )#嗯，这两个要求都需要成立！\n",
        "        \n",
        "\n",
        "        #leakage = sum( abs(C[n])**2 for n in range(2,max_lvl) )\n",
        "        \n",
        "        #print(\"Normalization_int: {}\" .format(sum(abs(C)**2)))\n",
        "        \n",
        "        return reward   \n",
        "\n",
        "    ## Reset initial state after every episode     \n",
        "    def reset(self):\n",
        "\n",
        "        self.r = random.randrange(self.N_asymmetries)            #随便找一个偏离对称的几何结构，即epsilon,或者,a,b\n",
        "        self.interp_energy = self.interp_energies[self.r]   #确定差值函数（generator）\n",
        "        self.interp_overlap = self.interp_overlaps[self.r]\n",
        "        \n",
        "        self.time = 0.0\n",
        "        self.alfa = 0.0\n",
        "        self.steps = 0\n",
        "        \n",
        "        self.protocol = []               \n",
        "        self.protocol.append(0)             #初始0时刻alpha为零\n",
        "        self.action_sequence = []\n",
        "\n",
        "        self.cum_reward = 0.0\n",
        "                \n",
        "        self.state =  [self.alfa/self.max_alfa] + [self.time/self.t_max]  #都是零，为何这样赋值？\n",
        "        \n",
        "        return np.array(self.state)       \n",
        "    \n",
        "    ## Take one step, and give reward as feedback.\n",
        "    def step(self, action): \n",
        "        \n",
        "        self.action_sequence.append(action)\n",
        "        self.time += self.alfa_dt  #真实时间\n",
        "        self.dalfa = self.action_space[action]  \n",
        "                \n",
        "        self.new_alfa = self.alfa + self.dalfa*self.alfa_dt  \n",
        "        \n",
        "        if self.new_alfa < 0.0:\n",
        "            reward = -10\n",
        "        elif self.new_alfa > self.max_alfa:\n",
        "            reward = -10\n",
        "        else:\n",
        "            reward = 0\n",
        "\n",
        "        self.new_alfa = np.clip(self.new_alfa, 0.0, self.max_alfa)\n",
        "        \n",
        "        self.steps += 1     \n",
        "        self.protocol.append(self.new_alfa)\n",
        " \n",
        "    \n",
        "        self.alfa = self.new_alfa\n",
        "\n",
        "        next_state = [self.alfa/self.max_alfa] + [self.time/self.t_max] \n",
        "        \n",
        "        if self.steps == self.N_alfa:\n",
        "            \n",
        "#             N_t = 1000\n",
        "            reward += 100*self.interp_reward(self.N_t)*self.reward_distribution[self.r]  #reward_distribution的元素都是1，为何要有它？          \n",
        "            done = True            \n",
        "            #if  reward > self.best_reward:\n",
        "             #   self.best_protocol = self.protocol\n",
        "             #   self.best_reward = reward\n",
        "                #print('REWARD: {}'.format(reward))\n",
        "        else:\n",
        "            done = False\n",
        "         \n",
        "        \n",
        "        self.cum_reward += reward  #reward只有两种方式可以得到值，区域越出或者最后self.steps == self.N_alfa\n",
        "        \n",
        "        if done == True:\n",
        "            \n",
        "            self.scores.append(self.cum_reward)\n",
        "        \n",
        "        return np.array(next_state), reward, done, {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCCxtJIl-e7e"
      },
      "source": [
        "# Define agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QazVQ8hK-e7f"
      },
      "source": [
        "# Based on implementation in Keras Deep Learning Cookbook by Rajdeep Dua et al. \n",
        "\n",
        "class Agent:\n",
        "    \n",
        "    def __init__(self, state_space, action_space):\n",
        "        \n",
        "        self.state_space = state_space\n",
        "        self.action_space = action_space\n",
        "        \n",
        "        self.memory = deque(maxlen = 50000)  #用来存状态？\n",
        "        self.gamma = 1.0\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.99995          #这几个epsilon值是什么？ \n",
        "        self.epsilon_min = 0.05\n",
        "        \n",
        "        self.learning_rate = 0.001        \n",
        "        \n",
        "        self.model = self.build_model()\n",
        "        self.target_model = self.build_model()\n",
        "        \n",
        "    def build_model(self):\n",
        "        \n",
        "        model = Sequential([\n",
        "            Dense(24, input_dim = self.state_space.shape[0], activation = 'relu'), #这里的self.state_space应该是keras张量\n",
        "            Dense(48, activation = 'relu'),\n",
        "            Dense(24, activation = 'relu'),\n",
        "            Dense(self.action_space.shape[0], activation = 'linear')\n",
        "        ])\n",
        "        \n",
        "        model.compile(loss='mse', optimizer = Adam(lr = self.learning_rate))\n",
        "        \n",
        "        return model\n",
        "    \n",
        "    def get_action(self,state):\n",
        "        \n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(len(self.action_space)-1) #-1 可以删除\n",
        "        \n",
        "        policy = self.model.predict(state)  \n",
        "        best_action = np.argmax(policy[0])  #policy的shape是什么\n",
        "        return best_action\n",
        " \n",
        "    \n",
        "    def add_to_memory(self, state, action, reward, next_state, done):\n",
        "        \n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        \n",
        "\n",
        "    def fit_from_memory(self, batch_size):\n",
        "    \n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "\n",
        "        for state, action, reward, next_state, done in batch:\n",
        "            \n",
        "            target = self.target_model.predict(state)\n",
        "            \n",
        "            if done:\n",
        "                target[0][action] = reward\n",
        "                \n",
        "            else:\n",
        "                Q_future = max(self.target_model.predict(next_state)[0])\n",
        "                target[0][action] = reward + Q_future*self.gamma\n",
        "                \n",
        "            self.model.fit(state, target, epochs = 1, verbose = 0)   \n",
        "        \n",
        "\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE7n3HdX-e7f"
      },
      "source": [
        "# Set Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBUqc1Wl-e7g"
      },
      "source": [
        "# env = Szilard_env()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DsJgqCw-e7g",
        "outputId": "155089a0-3715-4415-b961-90da0beddef1"
      },
      "source": [
        "env = Szilard_env()\n",
        "state_space = env.state_space\n",
        "action_space = env.action_space\n",
        "\n",
        "batch_size = 32\n",
        "n_episodes = 40000\n",
        "tau = 1e-3\n",
        "\n",
        "agent = Agent(state_space, action_space)\n",
        "agent.epsilon_decay = (0.05)**(1/n_episodes)\n",
        "agent.memory = deque(maxlen = 200000)\n",
        "\n",
        "score = [None]*n_episodes\n",
        "\n",
        "output_dir = 'model_output/Model_6/'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "    \n",
        "print(\"Training on {} asymmetries in the range : {}\".format(env.N_asymmetries,env.asymmetries))\n",
        "print(\"Available actions: \\n{}\".format(env.action_space))\n",
        "print(\"T_max = {}, Number of steps = {}, dt = {}\".format(env.t_max, env.N_alfa, env.alfa_dt))\n",
        "print(\"Hyperparameters: memorysize = {}, tau = {}, batch_size = {}\".format(len(agent.memory),tau, batch_size))\n",
        "print(\"Total number of experiences = {}\".format(env.N_alfa*n_episodes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on 6 asymmetries in the range : [0.53, 0.532, 0.534, 0.536, 0.538, 0.54]\n",
            "Available actions: \n",
            "[[-10240]\n",
            " [ -5120]\n",
            " [ -2560]\n",
            " [ -1280]\n",
            " [  -640]\n",
            " [  -320]\n",
            " [  -160]\n",
            " [   -80]\n",
            " [   -40]\n",
            " [   -20]\n",
            " [     0]\n",
            " [    20]\n",
            " [    40]\n",
            " [    80]\n",
            " [   160]\n",
            " [   320]\n",
            " [   640]\n",
            " [  1280]\n",
            " [  2560]\n",
            " [  5120]\n",
            " [ 10240]]\n",
            "T_max = 5, Number of steps = 50, dt = 0.1\n",
            "Hyperparameters: memorysize = 0, tau = 0.001, batch_size = 32\n",
            "Total number of experiences = 2000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an5SG6Zk-e7h"
      },
      "source": [
        "# Interact with environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz0X79vn-e7i",
        "outputId": "a89ee196-e014-43cb-f65d-5a2d14a2c29d"
      },
      "source": [
        "done = False\n",
        "\n",
        "for e in range(n_episodes):\n",
        "    \n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, (1,-1))\n",
        "    \n",
        "    for t in range(1000):\n",
        "        \n",
        "        action = agent.get_action(state)\n",
        "        \n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "                \n",
        "        next_state = np.reshape(next_state, (1,-1))\n",
        "        \n",
        "        agent.add_to_memory(state, action, reward, next_state, done)\n",
        "        \n",
        "        state = next_state\n",
        "        \n",
        "        if done:\n",
        "            print('episode: {}/{}, score: {:.3}, epsilon: {:.2}'.format(e,n_episodes, env.cum_reward, agent.epsilon))\n",
        "            score[e] = env.cum_reward\n",
        "            break\n",
        "        \n",
        "        if len(agent.memory) > batch_size:\n",
        "            agent.fit_from_memory(batch_size)\n",
        "\n",
        "            target_weights = np.array(agent.target_model.get_weights())\n",
        "            current_weights = np.array(agent.model.get_weights())\n",
        "\n",
        "            agent.target_model.set_weights(current_weights*tau + target_weights*(1-tau))\n",
        "        \n",
        "    if agent.epsilon > agent.epsilon_min:\n",
        "        agent.epsilon *= agent.epsilon_decay\n",
        "    \n",
        "    if e % 100 == 0:\n",
        "        \n",
        "        agent.model.save_weights(output_dir + \"weights_\" + \"{:04d}\".format(e) + \".hdf5\")\n",
        "            "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 0/40000, score: -1e+02, epsilon: 1.0\n",
            "episode: 1/40000, score: -2e+02, epsilon: 1.0\n",
            "episode: 2/40000, score: -89.8, epsilon: 1.0\n",
            "episode: 3/40000, score: -1.1e+02, epsilon: 1.0\n",
            "episode: 4/40000, score: -79.9, epsilon: 1.0\n",
            "episode: 5/40000, score: -70.0, epsilon: 1.0\n",
            "episode: 6/40000, score: -1.1e+02, epsilon: 1.0\n",
            "episode: 7/40000, score: -88.0, epsilon: 1.0\n",
            "episode: 8/40000, score: -1.2e+02, epsilon: 1.0\n",
            "episode: 9/40000, score: -49.9, epsilon: 1.0\n",
            "episode: 10/40000, score: -12.9, epsilon: 1.0\n",
            "episode: 11/40000, score: -2.1e+02, epsilon: 1.0\n",
            "episode: 12/40000, score: -17.7, epsilon: 1.0\n",
            "episode: 13/40000, score: -1.6e+02, epsilon: 1.0\n",
            "episode: 14/40000, score: -60.0, epsilon: 1.0\n",
            "episode: 15/40000, score: 69.9, epsilon: 1.0\n",
            "episode: 16/40000, score: -1.4e+02, epsilon: 1.0\n",
            "episode: 17/40000, score: -1.3e+02, epsilon: 1.0\n",
            "episode: 18/40000, score: -2e+02, epsilon: 1.0\n",
            "episode: 19/40000, score: -1.9e+02, epsilon: 1.0\n",
            "episode: 20/40000, score: -80.0, epsilon: 1.0\n",
            "episode: 21/40000, score: -80.0, epsilon: 1.0\n",
            "episode: 22/40000, score: -1.5e+02, epsilon: 1.0\n",
            "episode: 23/40000, score: -70.0, epsilon: 1.0\n",
            "episode: 24/40000, score: -69.4, epsilon: 1.0\n",
            "episode: 25/40000, score: -70.0, epsilon: 1.0\n",
            "episode: 26/40000, score: -1.9e+02, epsilon: 1.0\n",
            "episode: 27/40000, score: -2.3e+02, epsilon: 1.0\n",
            "episode: 28/40000, score: -1.2e+02, epsilon: 1.0\n",
            "episode: 29/40000, score: -1e+02, epsilon: 1.0\n",
            "episode: 30/40000, score: -50.0, epsilon: 1.0\n",
            "episode: 31/40000, score: -2.6e+02, epsilon: 1.0\n",
            "episode: 32/40000, score: -19.8, epsilon: 1.0\n",
            "episode: 33/40000, score: -1.2e+02, epsilon: 1.0\n",
            "episode: 34/40000, score: -30.0, epsilon: 1.0\n",
            "episode: 35/40000, score: -30.0, epsilon: 1.0\n",
            "episode: 36/40000, score: -70.0, epsilon: 1.0\n",
            "episode: 37/40000, score: -1.2e+02, epsilon: 1.0\n",
            "episode: 38/40000, score: -1.3e+02, epsilon: 1.0\n",
            "episode: 39/40000, score: -1.59e+02, epsilon: 1.0\n",
            "episode: 40/40000, score: -1e+02, epsilon: 1.0\n",
            "episode: 41/40000, score: -99.7, epsilon: 1.0\n",
            "episode: 42/40000, score: -1.4e+02, epsilon: 1.0\n",
            "episode: 43/40000, score: -60.0, epsilon: 1.0\n",
            "episode: 44/40000, score: -80.0, epsilon: 1.0\n",
            "episode: 45/40000, score: -1.6e+02, epsilon: 1.0\n",
            "episode: 46/40000, score: -2.1e+02, epsilon: 1.0\n",
            "episode: 47/40000, score: -19.5, epsilon: 1.0\n",
            "episode: 48/40000, score: -29.9, epsilon: 1.0\n",
            "episode: 49/40000, score: -80.0, epsilon: 1.0\n",
            "episode: 50/40000, score: -18.9, epsilon: 1.0\n",
            "episode: 51/40000, score: -1.1e+02, epsilon: 1.0\n",
            "episode: 52/40000, score: -1.9e+02, epsilon: 1.0\n",
            "episode: 53/40000, score: -40.0, epsilon: 1.0\n",
            "episode: 54/40000, score: -0.0484, epsilon: 1.0\n",
            "episode: 55/40000, score: -1.08e+02, epsilon: 1.0\n",
            "episode: 56/40000, score: -1.3e+02, epsilon: 1.0\n",
            "episode: 57/40000, score: -1.5e+02, epsilon: 1.0\n",
            "episode: 58/40000, score: -1.39e+02, epsilon: 1.0\n",
            "episode: 59/40000, score: -1e+02, epsilon: 1.0\n",
            "episode: 60/40000, score: -31.4, epsilon: 1.0\n",
            "episode: 61/40000, score: -62.9, epsilon: 1.0\n",
            "episode: 62/40000, score: -1.7e+02, epsilon: 1.0\n",
            "episode: 63/40000, score: -1.79e+02, epsilon: 1.0\n",
            "episode: 64/40000, score: -50.0, epsilon: 1.0\n",
            "episode: 65/40000, score: -1.9e+02, epsilon: 1.0\n",
            "episode: 66/40000, score: -29.7, epsilon: 1.0\n",
            "episode: 67/40000, score: -1.4e+02, epsilon: 0.99\n",
            "episode: 68/40000, score: -2.1e+02, epsilon: 0.99\n",
            "episode: 69/40000, score: -1.1e+02, epsilon: 0.99\n",
            "episode: 70/40000, score: -2.37e+02, epsilon: 0.99\n",
            "episode: 71/40000, score: -29.3, epsilon: 0.99\n",
            "episode: 72/40000, score: -78.6, epsilon: 0.99\n",
            "episode: 73/40000, score: -90.0, epsilon: 0.99\n",
            "episode: 74/40000, score: -89.8, epsilon: 0.99\n",
            "episode: 75/40000, score: -29.9, epsilon: 0.99\n",
            "episode: 76/40000, score: -69.8, epsilon: 0.99\n",
            "episode: 77/40000, score: -1.4e+02, epsilon: 0.99\n",
            "episode: 78/40000, score: -2e+02, epsilon: 0.99\n",
            "episode: 79/40000, score: -1.22e+02, epsilon: 0.99\n",
            "episode: 80/40000, score: -1.1e+02, epsilon: 0.99\n",
            "episode: 81/40000, score: -16.8, epsilon: 0.99\n",
            "episode: 82/40000, score: -1.4e+02, epsilon: 0.99\n",
            "episode: 83/40000, score: 0.962, epsilon: 0.99\n",
            "episode: 84/40000, score: -2.3e+02, epsilon: 0.99\n",
            "episode: 85/40000, score: -60.0, epsilon: 0.99\n",
            "episode: 86/40000, score: -99.8, epsilon: 0.99\n",
            "episode: 87/40000, score: -1.1e+02, epsilon: 0.99\n",
            "episode: 88/40000, score: -2.5e+02, epsilon: 0.99\n",
            "episode: 89/40000, score: -1.3e+02, epsilon: 0.99\n",
            "episode: 90/40000, score: -90.0, epsilon: 0.99\n",
            "episode: 91/40000, score: -1.8e+02, epsilon: 0.99\n",
            "episode: 92/40000, score: -69.6, epsilon: 0.99\n",
            "episode: 93/40000, score: -1.9e+02, epsilon: 0.99\n",
            "episode: 94/40000, score: -88.6, epsilon: 0.99\n",
            "episode: 95/40000, score: -1.39e+02, epsilon: 0.99\n",
            "episode: 96/40000, score: -1.07e+02, epsilon: 0.99\n",
            "episode: 97/40000, score: -29.2, epsilon: 0.99\n",
            "episode: 98/40000, score: -1.21e+02, epsilon: 0.99\n",
            "episode: 99/40000, score: -1.28e+02, epsilon: 0.99\n",
            "episode: 100/40000, score: -90.0, epsilon: 0.99\n",
            "episode: 101/40000, score: -1.35e+02, epsilon: 0.99\n",
            "episode: 102/40000, score: -60.0, epsilon: 0.99\n",
            "episode: 103/40000, score: -80.0, epsilon: 0.99\n",
            "episode: 104/40000, score: -1.2e+02, epsilon: 0.99\n",
            "episode: 105/40000, score: -1.9e+02, epsilon: 0.99\n",
            "episode: 106/40000, score: -1.9e+02, epsilon: 0.99\n",
            "episode: 107/40000, score: -1.9e+02, epsilon: 0.99\n",
            "episode: 108/40000, score: -1.5e+02, epsilon: 0.99\n",
            "episode: 109/40000, score: -1.3e+02, epsilon: 0.99\n",
            "episode: 110/40000, score: -50.0, epsilon: 0.99\n",
            "episode: 111/40000, score: -80.0, epsilon: 0.99\n",
            "episode: 112/40000, score: -1e+02, epsilon: 0.99\n",
            "episode: 113/40000, score: -1.8e+02, epsilon: 0.99\n",
            "episode: 114/40000, score: -1.3e+02, epsilon: 0.99\n",
            "episode: 115/40000, score: -79.3, epsilon: 0.99\n",
            "episode: 116/40000, score: -1.2e+02, epsilon: 0.99\n",
            "episode: 117/40000, score: -1.8e+02, epsilon: 0.99\n",
            "episode: 118/40000, score: 71.4, epsilon: 0.99\n",
            "episode: 119/40000, score: -53.7, epsilon: 0.99\n",
            "episode: 120/40000, score: -1.5e+02, epsilon: 0.99\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-5a467a1ab5c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-84-950ce9f4e946>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;31m#             N_t = 1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterp_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_distribution\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#reward_distribution的元素都是1，为何要有它？\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;31m#if  reward > self.best_reward:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-84-950ce9f4e946>\u001b[0m in \u001b[0;36minterp_reward\u001b[0;34m(self, N_t)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mk1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minteraction_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malfa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_lvl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mk2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minteraction_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malfa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mdalfa\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_lvl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mk3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minteraction_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malfa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mdalfa\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_lvl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mk4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minteraction_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malfa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mdalfa\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_lvl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-84-950ce9f4e946>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mk1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minteraction_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malfa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_lvl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mk2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minteraction_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malfa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mdalfa\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_lvl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mk3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minteraction_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malfa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mdalfa\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_lvl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mk4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minteraction_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malfa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mdalfa\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_lvl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-84-950ce9f4e946>\u001b[0m in \u001b[0;36minteraction_int\u001b[0;34m(n, alfa_in, f)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minteraction_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malfa_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterp_overlap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malfa_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1j\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_lvl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdalfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtest\u001b[0m  \u001b[0;31m#根据我的推导，这里的interp_overlap[n][m](abs(alfa_in))=<psi(n)|dH/dt|psi(m)>/(E_n-E_m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m##问题是，作为函数变量的alfa_in怎么能提供dH/dt里面的alpha对时间求导项？最后乘的dalfa？  这里需要乘一个\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-84-950ce9f4e946>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minteraction_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malfa_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterp_overlap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malfa_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1j\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_lvl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdalfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtest\u001b[0m  \u001b[0;31m#根据我的推导，这里的interp_overlap[n][m](abs(alfa_in))=<psi(n)|dH/dt|psi(m)>/(E_n-E_m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m##问题是，作为函数变量的alfa_in怎么能提供dH/dt里面的alpha对时间求导项？最后乘的dalfa？  这里需要乘一个\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/PythonData/lib/python3.6/site-packages/scipy/interpolate/polyint.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \"\"\"\n\u001b[1;32m     78\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finish_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/PythonData/lib/python3.6/site-packages/scipy/interpolate/interpolate.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, x_new)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;31m#    The behavior is set by the bounds_error variable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0mx_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         \u001b[0my_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extrapolate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0mbelow_bounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabove_bounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/PythonData/lib/python3.6/site-packages/scipy/interpolate/interpolate.py\u001b[0m in \u001b[0;36m_call_spline\u001b[0;34m(self, x_new)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_spline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_nan_spline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/PythonData/lib/python3.6/site-packages/scipy/interpolate/_bsplines.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, nu, extrapolate)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_c_contiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextrapolate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_shape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/PythonData/lib/python3.6/site-packages/scipy/interpolate/_bsplines.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, xp, nu, extrapolate, out)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextrapolate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         _bspl.evaluate_spline(self.t, self.c.reshape(self.c.shape[0], -1),\n\u001b[0;32m--> 361\u001b[0;31m                 self.k, xp, nu, extrapolate, out)\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ensure_c_contiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZSZ8woU-e7j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrwzB4ly-e7k",
        "outputId": "7fee17c6-b904-441c-8541-05edfba462bf"
      },
      "source": [
        "a.my"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dog'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8AzlwyR-e7m",
        "outputId": "ad0ec9bf-3203-4094-94f7-dc329d03fb99"
      },
      "source": [
        "a.h()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXCXenwJ-e7n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}